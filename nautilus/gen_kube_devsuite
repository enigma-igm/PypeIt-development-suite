#!/usr/bin/env python3
#
# See top-level LICENSE.rst file for Copyright information
#
# -*- coding: utf-8 -*-

"""
This script generates a kubernetes YAML file to run the PypeIt development suite 
"""

import shutil
import os
import io, yaml

from pkg_resources import resource_filename

from IPython import embed

def parser(options=None):
    import argparse

    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter,
                                     description='Generate a kubernetes YAML file for a Nautilus dev suite job')

    parser.add_argument('name', type=str, default=None, 
                        help='Name of the job (must consist of lower case alphanumeric characters, "-" or ".")')
    parser.add_argument('outfile', type=str, default=None, help='Name of the YAML outfile')
    parser.add_argument('-p', '--pypeit_branch', type=str, default='develop', help='Name of the PypeIt branch to test')
    parser.add_argument('-d', '--dev_branch', type=str, default='develop', help='Name of the PypeIt Dev-Suite branch to use')
    parser.add_argument('--ncpu', type=int, default=6, help='Number of CPUs request')
    parser.add_argument('--ram', type=int, default=50, help='Amount of RAM to request (Gi)')
    parser.add_argument('--coverage', default=False, action="store_true", help="Collect code coverage data.")
    parser.add_argument('--priority_list', default=False, action="store_true", help="Copy the test_priority_list to S3 after testing.")
    parser.add_argument('additional_args', type=str, nargs='*', default=["all"], help="Additional arguments to pypeit_test. Defaults to 'all'.")
    #parser.add_argument('-d', '--dryrun', default=False, action='store_true',
    #                    help='Only list the steps')

    return parser.parse_args() if options is None else parser.parse_args(options)


def main():

    pargs = parser()

    # Load the default
    def_yaml_file = os.path.join(os.getenv('PYPEIT_DEV'), 
                                 'nautilus', 
                                 'kube_dev_suite.yaml')
    with open(def_yaml_file, 'r') as stream:
        data = yaml.safe_load(stream)

    # Modify
    data['metadata']['name'] = pargs.name.lower()

    # Resources
    requests = data['spec']['template']['spec']['containers'][0]['resources']['requests']
    requests['cpu'] = str(pargs.ncpu)
    requests['memory'] = f'{pargs.ram}Gi'

    # Limits
    limits = data['spec']['template']['spec']['containers'][0]['resources']['limits']
    limits['cpu'] = str(pargs.ncpu + 1)
    limits['memory'] = f'{2*pargs.ram}Gi'

    ###### Args #####
    arguments = pargs.additional_args
    if pargs.coverage:
        arguments += ["--coverage", "coverage.report"]

    # Get PypeIt git up to date
    my_args = ' cd PypeIt;'
    my_args += ' git fetch;'
    my_args += f' git checkout {pargs.pypeit_branch};' 
    my_args += ' git pull --ff-only;'
    my_args += ' pip install -e ".[dev]";'
    # Telluric
    my_args += ' cd pypeit/data/telluric/atm_grids;'
    my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TelFit_MaunaKea_3100_26100_R20000.fits /tmp/telluric/TelFit_MaunaKea_3100_26100_R20000.fits --no-progress;'
    my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/telluric/atm_grids/TelFit_LasCampanas_3100_26100_R20000.fits /tmp/telluric/TelFit_LasCampanas_3100_26100_R20000.fits --no-progress;'
    my_args += ' ln -s /tmp/telluric/* .;'
    # Dev suite
    my_args += ' cd /tmp;'
    my_args += f' git clone --branch {pargs.dev_branch}  --depth 1 https://github.com/pypeit/PypeIt-development-suite.git;'
    my_args += ' cd PypeIt-development-suite;'
    my_args += ' source source_headless_test.sh;'
    # Pixel flat
    my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/CALIBS /tmp/CALIBS/ --recursive --no-progress;'
    my_args += ' ln -s /tmp/CALIBS CALIBS;'
    # Raw Data
    my_args += ' aws --endpoint $ENDPOINT_URL s3 cp s3://pypeit/RAW_DATA /tmp/RAW_DATA/ --recursive --no-progress;'
    my_args += ' ln -s /tmp/RAW_DATA RAW_DATA;'
    my_args += f' ./pypeit_test -t {pargs.ncpu} {" ".join(arguments)} -r pypeit.report -o /tmp/REDUX_OUT --csv performance.csv;'
    my_args += f' aws --endpoint $ENDPOINT_URL s3 cp pypeit.report s3://pypeit/Reports/{pargs.name}.report;'
    my_args += f' aws --endpoint $ENDPOINT_URL s3 cp performance.csv s3://pypeit/Reports/{pargs.name}_performance.csv;'
    if pargs.coverage:
        my_args += f' aws --endpoint $ENDPOINT_URL s3 cp coverage.report s3://pypeit/Reports/{pargs.name}.coverage.report;'
    if pargs.priority_list:
        my_args += f' aws --endpoint $ENDPOINT_URL s3 cp test_priority_list s3://pypeit/Reports/{pargs.name}.test_priority_list;'

    data['spec']['template']['spec']['containers'][0]['args'][0] = my_args

    with io.open(pargs.outfile, 'w', encoding='utf8') as outfile:
        yaml.dump(data, outfile, default_flow_style=False, allow_unicode=True)

    # Help
    print("\n\n=======================================")
    print("Helpful Hints:")
    print("=======================================")
    print(f"\n1) Launch the job with: \n\n kubectl -n pypeit create -f {outfile} \n")
    print(   "2) Monitor by going here: \n\n https://grafana.nrp-nautilus.io/d/85a562078cdf77779eaa1add43ccec1e/kubernetes-compute-resources-namespace-pods?orgId=1&refresh=10s&var-datasource=default&var-cluster=&var-namespace=pypeit \n")

if __name__ == '__main__':
    # Giddy up
    main()
